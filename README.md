# The SHAP method for interpretability of decisions on fraud detection obtained from machine learning models

The SHAP method (SHapley Additive exPlaination) is a modern approach for the local interpretation of machine learning models. Based on the Shapley Value, a concept introduced by Lloyd Shapley in 1953 in the field of cooperative game theory, the SHAP method allows each variable used in the training of a machine learning model to be assigned a value (positive or negative) corresponding to its importance in determining a certain result produced by that model. However, the computational complexity of this method grows exponentially as the number of variables used increases and, consequently, its implementation becomes prohibitively expensive for high-dimensional datasets. This has led to the development of approaches aimed at approximating its value while guaranteeing computational efficiency.

L’obbiettivo di questa tesi e quello di mostrare il funzionamento del metodo SHAP, delle sue estensioni e dei suoi metodi di approssimazione in relazione alle previsioni prodotte da alcuni modelli di machine
learning in un particolare contesto di high-stakes decision, quello della financial fraud detection. Così facendo si vuole mostrare in che modo questo metodo permetta di incrementare l’interpretabilita e quindi
l’affidabilita di questi modelli, nonche verificare i vantaggi computazionali comportati dai suoi approcci di approssimazione.
